{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !pip install mistral_inference\n",
    "### !pip install peft\n",
    "### Clone trl project https://github.com/huggingface/trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\n",
    "mistral_models_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 41120.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/root/mistral_models/7B-Instruct-v0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import datasets\n",
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from mistral_inference.model import Transformer\n",
    "from mistral_inference.generate import generate\n",
    "\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "from mistral_common.protocol.instruct.messages import UserMessage, SystemMessage, AssistantMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "\n",
    "\n",
    "tokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\n",
    "model = Transformer.from_folder(mistral_models_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_request = ChatCompletionRequest(messages=[SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "                                                      UserMessage(content=\"What is the best district in Paris? Answer exactly in 3 sentences.\"),])\n",
    "\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "torch.manual_seed(random.randint(0, 1000000))\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "out_tokens, _ = generate([tokens], model, max_tokens=512, temperature=0.8, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "result = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The \"best\" district in Paris can vary greatly depending on one's preferences, as each arrondissement (district) offers unique attractions, ambiance, and experiences. For those interested in historical landmarks and iconic sites, the 1st, 4th, and 6th arrondissements, which contain the Louvre, Notre-Dame, and the Latin Quarter, respectively, are popular choices. For a more bohemian and artistic atmosphere, the 5th and 18th arrondissements, home to the Luxembourg Gardens and Montmartre, respectively, are worth considering. Ultimately, the best district in Paris is the one that aligns with your personal interests and travel style.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 15.4k/15.4k [00:00<00:00, 13.6MB/s]\n",
      "Downloading data: 100%|██████████| 168M/168M [00:00<00:00, 234MB/s]  \n",
      "Downloading data: 100%|██████████| 25.9M/25.9M [00:00<00:00, 123MB/s] \n",
      "Downloading data: 100%|██████████| 240M/240M [00:00<00:00, 377MB/s]  \n",
      "Downloading data: 100%|██████████| 313M/313M [00:00<00:00, 454MB/s]  \n",
      "Downloading data: 100%|██████████| 9.99M/9.99M [00:00<00:00, 11.6MB/s]\n",
      "Downloading data: 100%|██████████| 182M/182M [00:00<00:00, 381MB/s]  \n",
      "Generating train split: 63967 examples [00:02, 29376.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dset = load_dataset(\"openbmb/UltraFeedback\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra = dset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 11834/63967 [00:00<00:00, 188501.24it/s]\n"
     ]
    }
   ],
   "source": [
    "res_dset = []\n",
    "for el in tqdm(list(ultra)):\n",
    "    if len(el['instruction']) < 128 and len(el['completions'][0]['response']) < 512 and len(el['completions'][1]['response']) < 512:\n",
    "        number_of_sentences = len(sent_tokenize(el['completions'][0]['response']))\n",
    "        right_answer = el['completions'][0]['response']\n",
    "        wrong_answer = el['completions'][1]['response']\n",
    "        res_dset.append({\n",
    "            \"prompt\": f\"{el['instruction']} You must generate exactly {number_of_sentences} sentences.\",\n",
    "            \"chosen\": [{'content': f\"{el['instruction']} You must generate exactly {number_of_sentences} sentences.\",\n",
    "                        'role': 'user'},\n",
    "                        {'content': right_answer,\n",
    "                         'role': 'assistant'}],\n",
    "            \"rejected\": [{'content': f\"{el['instruction']} You must generate exactly {number_of_sentences} sentences.\",\n",
    "                        'role': 'user'},\n",
    "                        {'content': wrong_answer,\n",
    "                         'role': 'assistant'}]\n",
    "        })\n",
    "    if len(res_dset) == 1024+256:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = res_dset[:1024]\n",
    "val_dset = res_dset[1024:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_to_upload_train = datasets.Dataset.from_list(train_dset)\n",
    "dset_to_upload_test = datasets.Dataset.from_list(val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict = DatasetDict({\n",
    "    \"train\": dset_to_upload_train,\n",
    "    \"test\": dset_to_upload_test\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddict.push_to_hub(\"<your_account>/exact_number_of_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.10 trl/examples/scripts/dpo.py --model_name_or_path mistralai/Mistral-7B-Instruct-v0.3 \\\n",
    "                                    --dataset_name <your_account>/exact_number_of_sentences --output_dir count_sentences \\\n",
    "                                    --report_to wandb --logging_steps 10 --per_device_train_batch_size 8 \\\n",
    "                                    --use_peft --learning_rate 1e-5  --bf16 --num_train_epochs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It will take only around 5-10 minutes to train such model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.51s/it]\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device=\"cuda:0\"\n",
    "\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "adapter_model_name = \"count_sentences\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model = PeftModel.from_pretrained(model, adapter_model_name)\n",
    "model = model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> What is the best district in Paris? Answer exactly in 3 sentences. The best district in Paris is the Marais, known for its vibrant nightlife, historic architecture, and trendy boutiques. The Latin Quarter, with its intellectual atmosphere, charming streets, and iconic landmarks like the Sorbonne and the Pantheon, is another great choice. Montmartre, famous for its bohemian history, Sacré-Cœur Basilica, and artistic ambiance, is a third excellent district.</s>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"What is the best district in Paris? Answer exactly in 3 sentences.\", return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_length=256, temperature=0.7, num_return_sequences=1)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
